---
title: 'Problem Set: Non Parametric Statistics: Group 18'
author: "Cesar Conejo Villalobos, Xavier Bryant"
date: "3/30/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

# Question 1

### Problem Description

Assess S claims of battery failure from temperatures from a sample of previous batteries that experienced failure and from a sample of past battery temperatures in general. 

a. Perform a kernel density estimation for temps-7 and temps-other using what you consider is the most adequate bandwidth. Since the temperatures are positive, is it required to perform any transformation?.

b. Is there any important difference on the results from considering the LSCV selector over the DPI selector?

c. It seems that in temps-7 there is a secondary mode. Compute a kernel derivative estimation for temps-7 and temps-other using what you consider are the most adequate bandwidths.

d. Precisely determine the location of the extreme points

e. Check with a kernel second derivative that the extreme points are actually modes.

## Preliminary Work

```{r, echo=FALSE}
problemPhones <- read.table("temps-7.txt", header = TRUE)$x
pastPhones    <- read.table("temps-other.txt", header = TRUE)$x
```

We see the summary of the "Problematic phone" series, phones that have burned due to excessive temperature, and then the "Past phone" series of the working temperature of general phones in the past. We notice that the problematic phones have a much higher maximum value but a similar median. This leads us to suspect that phones that do burn up, do more often, have higher temperatures that lead to their failure. 

```{r, echo=FALSE}
"Problematic phones"
summary(problemPhones)
"Past phones"
summary(pastPhones)
```

We can see comparing the histograms in figure \ref{fig:q1hist} that, as we saw in the summaries, the problematic phones have much more disperse temperatures and a longer tail in the direction of higher temperatures. This indeed leads us to suspect that phones that do fail have a section of their series that have higher temperatures. However, phones that fail have temperatures that are normally, for most of the sample, in line with those of the general past phones. 

```{r q1hist, echo=FALSE, fig.width = 12, fig.height = 4, fig.cap = "Comparison of histograms: Default bandwidths\\label{fig:q1hist}"}
# Histograms - default bandwidths
par(mfrow=c(1,2))
p1 <- hist(problemPhones, xlim=c(0,65), probability = T, col=rgb(1,0,0,1/4), main = "Problematic phones")
p2 <- hist(pastPhones, xlim=c(0,65), probability = T, col=rgb(0,0,1,1/4), main = "Past phones")
```

We see in our initial kernel densities with default bandwidths (Figure \ref{fig:q1density}) that there is a tail on the problematic series with a second mode for the high temperatures. There is also some distortion at the mode of the past battery curve, but the tail of high temperatures is not present.

```{r q1density, echo=FALSE, fig.width = 12, fig.height = 4, fig.cap = "Comparison of densities; Default bandwidths\\label{fig:q1density}"}
par(mfrow = c(1,2))
plot(density(x = problemPhones), xlim = c(-5,65), main = 'Problematic battery')
plot(density(x = pastPhones), xlim = c(-5,65), main = 'Past battery')
```

## Part a

### Problem Description 

Perform a kernel density estimation for temps-7 and temps-other using what you consider is the most adequate bandwidth.  Since the temperatures are positive, is it required to perform any transformation?.

### Results 

We can see these are the default bandwidths that are provided for the density estimates shown above. 

```{r, echo=FALSE}
"Problematic Temperatures"
round(bw.nrd0(x = problemPhones),3)
"Past Temperatures"
round(bw.nrd0(x = pastPhones),3)
```

#### Transformations

```{r, fig.show = 'hide', results='hide', echo=FALSE}
## kde with log-transformed data
kde_problemPhones <- density(log(problemPhones))
plot(kde_problemPhones, main = "Problematic battery: transformed data")
range(kde_problemPhones$x)

# Untransform kde$x so the grid is in (0, infty)
kde_transf_problemPhones <- kde_problemPhones
kde_transf_problemPhones$x <- exp(kde_transf_problemPhones$x)
# Transform the density using the chain rule
kde_transf_problemPhones$y <- kde_transf_problemPhones$y * 1 / kde_transf_problemPhones$x
```

Transformations must be performed in order to avoid boundary bias, which is to assign probability, with the function mapping, when there is no real density to support it or can be bias towards zero. We are looking at temperatures that can take values close to zero or very high values, therefore this would make us consider a log transformation. Below in Figure \ref{fig:q1transdensityproblematic} and Figure \ref{fig:q1transdensitypast}, we do not see the shape of the curve change substantially - there are not sections that are mapped by the function but not supported by any density - nor do we see much boundary bias. However, the log transformation does further smooth our data - which for our analysis comparing the overall distribution of batteries - is helpful. This is due to the smaller bandwidth that is enabled due to the transformation, which also advantageous due to asymptotic properties. However, the untransformed series functions as well for our purposes, as a more precise curve is not necessarily important for our general type of analysis, and we don't see significant bias. Therefore, we proceed with the regular series.


```{r q1transdensityproblematic, echo=FALSE, fig.width = 12, fig.height = 4, fig.cap = "Comparisono default and transformed densities for Problematic series\\label{fig:q1transdensityproblematic}"}
par(mfrow = c(1,2))
plot(density(x = problemPhones), xlim = c(-5,65), main = 'Problematic battery')
# Transformed kde
plot(kde_transf_problemPhones, main = "Problematic battery: Transformed", xlim = c(-5, 65))
par(mfrow = c(1,1))
```

```{r, fig.show = 'hide', results='hide', echo=FALSE}
## kde with log-transformed data
kde_pastPhones <- density(log(pastPhones))
plot(kde_pastPhones, main = "Kde of transformed data")
range(kde_pastPhones$x)

# Untransform kde$x so the grid is in (0, infty)
kde_transf_pastPhones <- kde_pastPhones
kde_transf_pastPhones$x <- exp(kde_transf_pastPhones$x)

# Transform the density using the chain rule
kde_transf_pastPhones$y <- kde_transf_pastPhones$y * 1 / kde_transf_pastPhones$x
```

```{r q1transdensitypast, echo=FALSE, fig.width = 12, fig.height = 4, fig.cap = "Comparison of default and transformed densities for Past series\\label{fig:q1transdensitypast}"}
par(mfrow = c(1,2))
plot(density(x = pastPhones), xlim = c(-5,60), main = 'Past battery')
# Transformed kde
plot(kde_transf_pastPhones, main = "Past battery: Transformed", xlim = c(-5, 60))
par(mfrow = c(1,1))
```

\newpage

#### Bandwidth Selection

We will use several techniques to select the optimal bandwidth. The first technique is "rule of thumb" bandwidth selection derived from minimizing the AMISE with a normal parametric assumption for the unknown $R(f'')$, in zero stage part of the AMISE (asympotic mean integrated squared error) derivation: $h_{AMISE} = [\frac{R(K)}{\mu^2_2(K)R(f'')n}]^\frac{1}{5}$. We then look at the Direct Plug In (DPI) selector, which instead of using a normal parameterization in the zero stage, the DPI buries the parametric assumption, usually, with two stages which balances between bias (lower with number of stages) and variance (higher with the number of stages). We then look at two cross-validation techniques, where we attempt to minimize the MISE (mean integrated squared error), through means of cross validation utilizing leave-one-out techniques. The two types of CV analysis are: the Least Squared Cross-Validation (or Unbiased Cross Validation) and the Biased Cross Validation. Numerical optimization is required for the LSCV and therefore we can get trapped in spurious solutions, necessitating the limiting of the bandwidth grid for the $\hat{h}_{LSCV}$. The BCV adapts a hybrid strategy estimating a modification of $R(f'')$ with leave-out-diagonals. It is important that the plot or the grid is reviewed in this selector as well as we are looking for the local minimizer not the global as h, the bandwidth , goes to infinity. We ensure to limit the bandwidth grid for both these functions, when necessary, from the function shown in class.

First we see the bandwidths of the "rule of thumb" selector:

```{r, echo=FALSE}
# RT
"Past temperatures"

round(bw.nrd(x = pastPhones),3)

"Problematic temperatures"

round(bw.nrd(x = problemPhones),3)
```

Second we see the DPI bandwidths:

```{r, echo=FALSE}
# DPI

"Past temperatures"

round(ks::hpi(x = pastPhones),3) 

"Problematic temperatures"

round(ks::hpi(x = problemPhones),3)
```

Third, we have the LSCV bandwidths:

```{r, echo=FALSE}
# LSCV 

#Class function

bw.ucv.mod <- function(x, nb = 1000L,
                       h_grid = 10^seq(-3, log10(1.2 * sd(x) *
                                                   length(x)^(-1/5)), l = 200),
                       plot_cv = FALSE) {
  if ((n <- length(x)) < 2L)
    stop("need at least 2 data points")
  n <- as.integer(n)
  if (is.na(n))
    stop("invalid length(x)")
  if (!is.numeric(x))
    stop("invalid 'x'")
  nb <- as.integer(nb)
  if (is.na(nb) || nb <= 0L)
    stop("invalid 'nb'")
  storage.mode(x) <- "double"
  hmax <- 1.144 * sqrt(var(x)) * n^(-1/5)
  Z <- .Call(stats:::C_bw_den, nb, x)
  d <- Z[[1L]]
  cnt <- Z[[2L]]
  fucv <- function(h) .Call(stats:::C_bw_ucv, n, d, cnt, h)
  ## Original
  # h <- optimize(fucv, c(lower, upper), tol = tol)$minimum
  # if (h < lower + tol | h > upper - tol)
  #   warning("minimum occurred at one end of the range")
  ## Modification
  obj <- sapply(h_grid, function(h) fucv(h))
  h <- h_grid[which.min(obj)]
  if (h %in% range(h_grid)) 
    warning("minimum occurred at one end of h_grid")
  if (plot_cv) {
    plot(h_grid, obj, type = "o")
    rug(h_grid)
    abline(v = h, col = 2, lwd = 2)
  }
  h
}

"Past temperatures" 

bw.ucv.mod(x = pastPhones, h_grid = 10^seq(-1.5, 0.5, l = 200))

"Problem temperatures"

bw.ucv.mod(x = problemPhones, h_grid = 10^seq(-1.5, 0.5, l = 200))
```

Lastly we have the BCV bandwidths:

```{r, echo=FALSE}
# BCV
# Class function

bw.bcv.mod <- function(x, nb = 1000L,
                       h.grid = 10^seq(-3, log10(1.2 * sd(x) *
                                                   length(x)^(-1/5)), l = 200),
                       plot.cv = FALSE) {
  if ((n <- length(x)) < 2L)
    stop("need at least 2 data points")
  n <- as.integer(n)
  if (is.na(n))
    stop("invalid length(x)")
  if (!is.numeric(x))
    stop("invalid 'x'")
  nb <- as.integer(nb)
  if (is.na(nb) || nb <= 0L)
    stop("invalid 'nb'")
  storage.mode(x) <- "double"
  hmax <- 1.144 * sqrt(var(x)) * n^(-1/5)
  Z <- .Call(stats:::C_bw_den, nb, x)
  d <- Z[[1L]]
  cnt <- Z[[2L]]
  fbcv <- function(h) .Call(stats:::C_bw_bcv, n, d, cnt, h)
  # h <- optimize(fbcv, c(lower, upper), tol = tol)$minimum
  # if (h < lower + tol | h > upper - tol)
  #   warning("minimum occurred at one end of the range")
  obj <- sapply(h.grid, function(h) fbcv(h))
  h <- h.grid[which.min(obj)]
  if (h %in% range(h.grid)) 
    warning("minimum occurred at one end of h.grid")
  if (plot.cv) {
    plot(h.grid, obj, type = "o")
    rug(h.grid)
    abline(v = h, col = 2, lwd = 2)
  }
  h
}

"Past temperatures"

bw.bcv.mod(x = pastPhones  , nb = 1000L)

"Problematic temperatures"

bw.bcv.mod(x = problemPhones  , nb = 1000L)
```
To summarize we have:

Past temperature bandwidths:
   - RT: 1.15
   - DPI: 0.84
   - LSCV:  0.64
   - BCV: 0.88

Problematic temperature bandwidths:
   - RT: 0.87 
   - DPI: 0.63
   - LSCV: 0.66
   - BCV:  0.61

Reviewing our results, we see that that the RT gives us bandwidths that are quite large in comparison with the other bandwidth selectors, which is common for non-normal data like our own. From the literature, we know from  the DPI selector has a convergence rate that is much faster than the cross-validation technique, and therefore is dominant among academics. We also note that the BCV tends to be more bias, but have substantially less variance than the LSCV, tending to have larger bandwidths.  However, we notice that the default values for bandwidth of 0.75 for the problematic series and, for the past series, 0.98 are fairly similar and, considering part b, we decide to use the default bandwidths as, for our purpose of comparing the series at its entirety, the differences are not important.

## Part b

### Problem Description

Is there any important difference on the results from considering the LSCV selector over the DPI selector?

### Results 

Bandwidths for LSCV and BCV:

Past temperature bandwidths:
   - DPI: 0.84
   - LSCV:  0.64

Problematic temperature bandwidths:
   - DPI: 0.63
   - LSCV: 0.66

We can see that the bandwidths of LSCV and DPI for the past temperatures differ significantly more than those for the problematic temperatures. The difference between the bandwidths is only 0.03 for the problematic temperatures. It is 0.2 for the past temperatures. In the literature, the DPI has much higher convergence rate than cross-validation methods, like LSCV, however, with very volatile or non-normal data the DPI may over smooth. We may be seeing this in practice as the LSCV does have a smaller bandwidth that is capturing more of disruption around the mode in the past temperatures series. The sample size is also considerably larger in the problematic temperature series so this may allow the two techniques to more closely approach one another. For functional purposes, they are fairly equivalent to us as we are comparing the overall past and problematic series, and both bandwidths more or less find similar curves.

```{r, echo=FALSE}
#LSCV

#Past temperatures

bw.lscv.pastPhones <- bw.ucv(x = pastPhones)

#Problem Temperatures

bw.lscv.problemPhones <- bw.ucv(x = problemPhones)

#DPI

#Past temperatures

bw_dpi_pastPhones <- bw.SJ(x = pastPhones, method = "dpi")

#Problem Temperatures

bw_dpi_problemPhones <- bw.SJ(x = problemPhones, method = "dpi")
```

```{r q1partbprob, echo=FALSE, fig.width = 12, fig.height = 4, fig.cap = "LSCV and DPI densities problematic series\\label{fig:q1partbprob}"}
# Problem Temperatures
par(mfrow=c(1,2))
plot(density(x = problemPhones, bw = bw.lscv.problemPhones), xlim = c(-5,65), main = "Density LSCV: Problematic")
plot(density(x = problemPhones, bw = bw_dpi_problemPhones), xlim = c(-5,65), main = "Density DPI: Problematic")
```

```{r q1partbpast, echo=FALSE, fig.width = 12, fig.height = 4, fig.cap = "LSCV and DPI densities past series\\label{fig:q1partbpast}"}
# Past Temperatures
par(mfrow=c(1,2))
plot(density(x = pastPhones, bw = bw.lscv.pastPhones), xlim = c(-5,65), main = "Density LSCV: Past") 
plot(density(x = pastPhones, bw = bw_dpi_pastPhones), xlim = c(-5,65), main = "Density DPI: Past")
```

\newpage

## Part c

### Problem Description

It seems that in temps-7 there is a secondary mode. Compute a kernel derivative estimation for temps-7 and temps-other using what you consider are the most adequate bandwidths.

### Results 

We can see indeed that there is two modes in the problematic series. This second mode is key for our analysis as it likely shows that this mode of high temperatures could be largely responsible for the failure of these cellphones rather than those in general, or the past temperature series. There is further evidence they are modes as the the first derivative crosses the x-axis at these points. We also see a minimum in red. 

```{r q1partcprob, echo=FALSE, fig.width = 12, fig.height = 4, fig.cap = "Mode and derivative estimation analysis: Problematic series\\label{fig:q1partcprob}"}
## Detect Global mode
posMax1 <- which.max(density(x = problemPhones)$y)

## Detect minima
YY <- density(x = problemPhones)$y[density(x = problemPhones)$x > 20 & density(x = problemPhones)$x < 40]
minY <- min(YY)
posMin <- which(density(x = problemPhones)$y == minY)

## Detect Secondary mode
YY <- density(x = problemPhones)$y[density(x = problemPhones)$x > 33 & density(x = problemPhones)$x < 60]
maxY <- max(YY)
posMax2 <- which(density(x = problemPhones)$y == maxY)

kdde_0_problemPhones <- ks::kdde(x = problemPhones, deriv.order = 0)
kdde_1_problemPhones <- ks::kdde(x = problemPhones, deriv.order = 1)

par(mfrow = c(1,2))
plot(kdde_0_problemPhones, xlab = "x", main = "Mode analysis: Problematic", xlim = c(-5,65))
abline(v = density(x = problemPhones)$x[posMax1], col = "3")
abline(v = density(x = problemPhones)$x[posMax2], col = "3")
abline(v = density(x = problemPhones)$x[posMin], col = "2")
plot(kdde_1_problemPhones, xlab = "x", main = "Density derivative: Problematic")
abline(v = density(x = problemPhones)$x[posMax1], col = "3")
abline(v = density(x = problemPhones)$x[posMax2], col = "3")
abline(v = density(x = problemPhones)$x[posMin], col = "2")
abline(h = 0)
par(mfrow = c(1,1))
```


We can see the mode identified in the past phone series with the green line. We can see that the first derivative crosses the x-axis several times reflecting the disturbance we have around the mode of the series. However, we can still see that a majority of the data falls in the region around 10 and 20. This general idea is suitable for our analysis. 

```{r q1partcpast, echo=FALSE, fig.width = 12, fig.height = 4, fig.cap = "Mode and derivative estimation analysis: Past series\\label{fig:q1partcpast}"}
pos_Max_PastPhone <- which.max(density(x = pastPhones)$y)

kdde_0_pastPhones <- ks::kdde(x = pastPhones, deriv.order = 0)
kdde_1_pastPhones <- ks::kdde(x = pastPhones, deriv.order = 1)

par(mfrow = c(1,2))
plot(kdde_0_pastPhones, xlab = "x", main = "Mode analysis: Past", xlim = c(-5,65))
abline(v = density(x = pastPhones)$x[pos_Max_PastPhone], col = "3")
plot(kdde_1_pastPhones, xlab = "x", main = "Density derivative: Past")
abline(v = density(x = pastPhones)$x[pos_Max_PastPhone], col = "3")
abline(h = 0)
par(mfrow = c(1,1))
```

## Part d

### Problem Description

Precisely determine the location of the extreme points.

### Results

We can see the extreme points of the problematic series for the two modes (13.72, 0.08) and (41.53, 0.01) as well as the minimum point (33.83, 0.002). For the past series, we can see the mode at (31.36, 0.007).

```{r q1partcpast2, echo=FALSE, fig.width = 12, fig.height = 4, fig.cap = "Mode and derivative estimation analysis: Past phone series\\label{fig:q1partcpast2}"}
"Problematic temperatures"

"Mode 1: x-value"
density(x = problemPhones)$x[posMax1]  
"Mode 2: x-value"
density(x = problemPhones)$x[posMax2]
"Minimum: x-value"
density(x = problemPhones)$x[posMin]

"Mode 1: y-value"
density(x = problemPhones)$y[posMax1]
"Mode 2: y-value"
density(x = problemPhones)$y[posMax2] 
"Minimum: y-value"
density(x = problemPhones)$y[posMin]

"Past temperatures"

"Mode 1: x-value"
density(x = pastPhones)$x[posMax2]

"Mode 1: y-value"
density(x = pastPhones)$y[posMax2]  
```

## Part e

### Problem Description

Check with a kernel second derivative that the extreme points are actually modes.

### Results

We can confirm that the points of the modes for the problematic series are indeed the modes as the they remain on the opposite side of the second derivative estimator. A mode should be on the negative side of the second derivative and a minimum should be on the positive side. We see this is true for both the modes and minimum of the problematic series.

```{r q1partepast, echo=FALSE, fig.width = 12, fig.height = 4, fig.cap = "Second derivative analysis: Problematic series\\label{fig:q1partepast}"}
kdde_2_problemPhones <- ks::kdde(x = problemPhones, deriv.order = 2)
par(mfrow = c(1,3))
plot(kdde_0_problemPhones, xlab = "x", main = "Problematic series: \n Density estimation", xlim = c(-5,65))
abline(v = density(x = problemPhones)$x[posMax1], col = "3")
abline(v = density(x = problemPhones)$x[posMax2], col = "3")
abline(v = density(x = problemPhones)$x[posMin], col = "2")
plot(kdde_1_problemPhones, xlab = "x", main = "Problematic series: \n Density derivative estimation")
abline(v = density(x = problemPhones)$x[posMax1], col = "3")
abline(v = density(x = problemPhones)$x[posMax2], col = "3")
abline(v = density(x = problemPhones)$x[posMin], col = "2")
abline(h = 0)
plot(kdde_2_problemPhones, xlab = "x", main = "Problematic series: \n  Density second derivative estimation")
abline(v = density(x = problemPhones)$x[posMax1], col = "3")
abline(v = density(x = problemPhones)$x[posMax2], col = "3")
abline(v = density(x = problemPhones)$x[posMin], col = "2")
abline(h = 0)
par(mfrow = c(1,1))
```

We see this is also true for the past series, where the mode is on the negative side of the second derivative estimator.

```{r q1parteprob, echo=FALSE, fig.width = 12, fig.height = 4, fig.cap = "Second derivative analysis: Past series\\label{fig:q1parteprob}"}
kdde_2_pastPhones <- ks::kdde(x = pastPhones, deriv.order = 2)

par(mfrow = c(1,3))
plot(kdde_0_pastPhones, xlab = "x", main = "Past series \n Density estimation", xlim = c(-5,65))
abline(v = density(x = pastPhones)$x[pos_Max_PastPhone], col = "3")
plot(kdde_1_pastPhones, xlab = "x", main = "Past battery \n Density derivative estimation")
abline(v = density(x = pastPhones)$x[pos_Max_PastPhone], col = "3")
abline(h = 0)
plot(kdde_2_pastPhones, xlab = "x", main = "Past series \n Density Second derivative estimation")
abline(v = density(x = pastPhones)$x[pos_Max_PastPhone], col = "3")
abline(h = 0)
par(mfrow = c(1,1))
```

\newpage

# Question 2

### Problem Description

Compare the MISE and AMISE criteria in three densities in a `nor1mix` of your choice.

1. Code (2.33) and the AMISE expression for the normal kernel, and compare the two error curves.

2. Compare them for $n = 100, 200, 500$, adding a vertical line to represent the $h_{MISE}$ and $h_{AMISE}$ bandwidths. Describe in detail the results and the major takeaways.

### Response


For this exercise, we require some mathematical ideas that we will develop briefly.

We start with the KDE estimator $\hat{f}(x;h) = \sum_{i = 1}^{n} K_{h}(x - X_{i})$. The expectation and variance for this estimator are given by the following expressions:

(1) $E[\hat{f}(x;h)] = (K_{h} * f)(x)$.

(2) $\text{Var}[\hat{f}(x;h)] = \frac{1}{n}{((K_{h}^{2}*f)(x) - {(K_{h} * f)}^{2}(x))}$

Then, we develop some asymptotic expressions for (1) and (2):

(3) $E[\hat{f}(x;h)] - f(x) = \text{Bias}[\hat{f}(x;h)] = \frac{1}{2}\mu_{2}(K)f^{''}(x)h^{2} + o(h^{2})$

(4) $\text{Var}[\hat{f}(x;h)] = \frac{R(K)}{nh}f(x) + o({(nh)}^{-1})$

Then, from equations (3) and (4) we obtain the following expression for the MSE:

(5) $\text{MSE}[\hat{f}(x;h)] = \frac{\mu_{2}^{2}(K)}{4}{(f^{''}(x))}^{2}h^{4} + \frac{R(K)}{nh}f(x) + o(h^{4} + {(nh)}^{-1})$

It is important to note that in (3), (4) and (5) we define $K$ and $R(K)$ as:

(6) Second order moment of $K$: $\mu_{2}(K) := \int z^{2}K(z) dz$

(7) Squared integral of kernel: $R(K) := \int {(K(x))}^{2} dx$

We are able to define $\text{MISE}[\hat{f}(\cdot;h)]$ as global error criteria for measuring the performance of $\hat{f}$ in relation to the target density $f$ by taking the 
the intergral of the of MSE.

(8) $\text{MISE}[\hat{f}(\cdot;h)] = \int \text{MSE}[\hat{f}(x;h)]$ 

Therefore, we obtain the following asymptotic expansion for the MISE:

(9) $\text{MISE}[\hat{f}(\cdot;h)] = \frac{1}{4} \mu_{2}^{2}(K)R(f^{''})h^{4} + \frac{R(K)}{nh} + o(h^{4} + {(nh)}^{-1})$

We define the dominant part of equation (9) (the little $o$ will asymptotically approach zero more quickly) as $\text{AMISE}[\hat{f}(\cdot;h)]$. In particular:

(10) $\text{AMISE}[\hat{f}(\cdot;h)] = \frac{1}{4} \mu_{2}^{2}(K)R(f^{''})h^{4} + \frac{R(K)}{nh}$

with the expression $R(f^{''})$ given by:

(11) $R(f^{''}) = \int {(f^{''}(x))}^{2} dx$

We can see, finally, the bandwidth that minimizes the AMISE is:

(12) $h_{AMISE} = {\left[ {\frac{R(K)}{\mu_{2}^{2}(K)R(f^{''})n}} \right]}^{1/5}$

Now, we consider our particular case of study. In this case, we use the following assumptions to *reduce* our analysis and proceed in our analysis:

a) A normal kernel $K_{h}(\cdot)$ with distribution $\mathcal{N}(0,1)$

b) The density function $f$ is based on the family of normal $r$-mixtures, therefore we obtain: 

(13) $f(x; \mu, \sigma, \boldsymbol{w}) = \sum_{j = 1}^{r} w_{j}\phi_{\sigma_{j}}(x - \mu_{j})$

where $w_{j} \geq 0$, $j = 1,...,r$ and $\sum_{j=1}^{r} w_{j} = 1$.

With these two expressions, we can obtain a specific value for the AMISE in equation (10). Utilizing assumption a), we obtain the following expressions for equations (6) and (7):

(6.1) Second order moment of $K$: $\mu_{2}(K) =  1$

(7.1) Squared integral of kernel: $R(K) =  \frac{1}{2 \sqrt{\pi}}$

Expression for equation (11) can also be derived following  *Theorem 4.1* from *Marron and Wand (1992)*

(11.1) $R(f^{''}) = \int {(f^{''}(x))}^{2} dx = \sum_{j=1}^{k} \sum_{j'=1}^{k} w_{j}w_{j'}\phi_{\sigma_{jj'}}^{4}(\mu_{j} - \mu_{j'})$

Where $\sigma_{jj'} = {(\sigma_{j}^{2} + \sigma_{j'}^{2})}^{1/2}$. Additionally, we use the *Probabilist Hermite Polynomial* of order 4: $\phi^{4}(x) = \phi(x)H_{4}(x) = \phi(x)(x^{4} - 6x^{2} + 3)$. With this expression, we obtain the reduced form of the AMISE:

(10.1) $\text{AMISE}[\hat{f}(\cdot;h)] = \frac{1}{4}R(f^{''})h^{4} + \frac{1}{2nh\sqrt{\pi}}$

With optimal bandwidth $h_{AMISE}$ given by:

(12.1) $h_{AMISE} = {\left[ {\frac{{(2 \sqrt{\pi})}^{-1}}{R(f^{''})n}} \right]}^{1/5}$

Finally, under this assumptions, we obtain a explicit and exact MISE expression of equation (8):

(14) $\text{MISE}_{r}[\hat{f}(\cdot;h)] = {(2\sqrt{\pi}nh)}^{-1} + \boldsymbol{w}^{'}\{(1 - n^{-1})\Omega_{2} - \Omega_{1} + \Omega_{0}\}\boldsymbol{w}$

Where:

(15) $(\Omega_{a})_{i,j} = \phi_{{(ah^{2} + \sigma_{i}^{2} + \sigma_{j}^{2})}^{1/2}}(\mu_{i} - \mu_{j})$ for $i,j = 1,...,r$

Finally, we can proceed evaluating numerically with equation (14) and obtain:

(16) $\text{arg min}_{h > 0} \text{MISE}[\hat{f}(\cdot;h)]$

With this mathematical review, we are able to compare the MISE and AMISE criteria. We code the following functions in `R`:

* `omega_a()`: Computes the matrix  $(\Omega_{a})_{i,j}$ defined in equation (15).

* `omega()`: Computes the `scalar` given by the following vector and matrix operations of equation (14): $\Omega = \boldsymbol{w}^{'}\{(1 - n^{-1})\Omega_{2} - \Omega_{1} + \Omega_{0}\}\boldsymbol{w}$

* `MISE()`: Useful for computing the expression of equation (14).

* `AMISE()`: Useful for computing the expression of equation (10.1). We also code the auxiliary functions `Hermite_4()` and `R_f2()` related to equation (11.1).

* Value of $h_{\text{AMISE}}$ is obtained from the code function `h_AMISE_n()`. Value of $h_{\text{MISE}}$ is obtained numerically from equations (15) and (15) using `optimize()`.

We consider in particular the following three densities of the `nor1mix` package whose parameter are available in [R Help](https://marronwebfiles.sites.oasis.unc.edu/OldResearch/parameters/nmpar.m)

* `MW.nm1`: Gaussian  $\mathcal{N}(0,1)$

* `MW.nm2`: Skewed $.2 \mathcal{N}(-.3, 1.44) + .2 \mathcal{N}(.3, .64) + .6 \mathcal{N}(1, 4/9)$

* `MW.nm6`: Bimodal $.5 \mathcal{N}(-1, 4/9) + .5 \mathcal{N}(1, 4/9)$

In the case  using the data set `nor1mix::MW.nm1`, figure \ref{fig:q2plot0} shows the histograms for the sample distribution for $n = 100, 200, 500$ and shows the population value in the red density curve. As expected, when we increase the number of observations the sample histogram approaches the distribution of the population density curve.

```{r eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
# Charge to the environment.Do not show results and code

# Functions =================================

## a) Omega_a

omega_a <- function(a, w, h, mu, sigma2){
  k <- length(w)
  M <- matrix(0, ncol = k, nrow = k)
  for(i in 1:k){
    for(j in 1:k){
      M[i,j] = dnorm(x = (mu[i] - mu[j]), 
                     mean = 0, 
                     sd = sqrt(a*h^2 + sigma2[i] + sigma2[j]))
      
    } #End for j
  } # End for i
  return(M)
}

## a) Omega

omega <- function(w, h, n, mu, sigma2){
  
  omega2 <- omega_a(a = 2, w, h, mu, sigma2)
  omega1 <- omega_a(a = 1, w, h, mu, sigma2)
  omega0 <- omega_a(a = 0, w, h, mu, sigma2)
  omega <- (1 - (1/n))*omega2 - 2*omega1 + omega0
  return(t(w) %*% omega %*% w)
}

## c) Explicit MISE 

MISE <- function(h, n, w, mu, sigma2){
  
  len_h   <- length(h)
  MISE_h1 <- numeric(len_h)
  MISE_h  <- numeric(len_h)
  for(i in 1:len_h){MISE_h1[i] <- omega(w, h[i], n, mu, sigma2)}
  MISE_h <- (2*sqrt(pi) *n*h)^(-1) + MISE_h1
  return(MISE_h)
  
}

## d) Explicit AMISE

## Inputs
## n: Number of sample observations
## h: Grid of bandwidth 
## R_K: Squared integral of kernel:
## R_f2: Second derivative of Squared integral of density f

AMISE <- function(n, h, R_K, R_f2){((n*h)^(-1))*R_K + 0.25*(h^4)*R_f2}

## e) Optimal AMISE bandwidth

## Inputs
## n: Number of sample observations.
## R_K: Squared integral of kernel.
## R_f2: Second derivative of Squared integral of density f

h_AMISE_n <- function(n, R_K, R_f2){(R_K/(n*R_f2))^(1/5)}

## f) Hermite Polynomial: Useful for the four derivative of Normal density

Hermite_4 <- function(x, mean, sd){
  
  H4 <- x^4 - 6*x^2 + 3
  Normal4 <-  dnorm(x = x, 
                    mean = mean, 
                    sd = sd)
  
  return(Normal4*H4)
  
}

## g) Comute the Second derivative of Squared integral of density f (Normal mixture)

## Inputs

## w: vector of weights
## mu: Vector of means
## sigma2: Vector of variance

R_f2 <- function(w, mu, sigma2){
  
  k <- length(w)
  sum_int <- 0
  for(i in 1:k){
    for(j in 1:k){
      sum_int <- sum_int + (w[i]*w[j]*Hermite_4(x = (mu[i] - mu[j]), 
                                                mean = 0, 
                                                sd = sqrt(sigma2[i] + sigma2[j])))
    }
  }
  return(sum_int)
}

# Inputs: =================================
# Useful for all scenarios

# Density evaluation
x <- seq(-4, 4, length.out = 400)

# h_grid and Second derivative of the normal kernel
h <- seq(0.04,0.85, length.out = 100)
R_K_N01 <- (2*sqrt(pi))^(-1)
R_f2_f01 <- (3/(8*sqrt(pi)))

# Simulating
set.seed(42)
x100 <- nor1mix::rnorMix(n = 100, obj = nor1mix::MW.nm1)
x200 <- nor1mix::rnorMix(n = 200, obj = nor1mix::MW.nm1)
x500 <- nor1mix::rnorMix(n = 500, obj = nor1mix::MW.nm1)

# AMISE n = 100
AMISE_100 <- AMISE(n = 100, h = h, R_K = R_K_N01, R_f2 = R_f2_f01)
h_AMISE_100 <- h_AMISE_n(n = 100, R_K = R_K_N01, R_f2 = R_f2_f01)

# MISE = 100
MISE_100 <- MISE(h, n = 100, w = 1, mu = 1, sigma2 = 1)
h_MISE_100 <- optimize(MISE, c(0, 1), tol = 0.0001, n = 100, w = 1, mu = 1, sigma2 = 1)

# AMISE n = 200
AMISE_200 <- AMISE(n = 200, h = h, R_K = R_K_N01, R_f2 = R_f2_f01)
h_AMISE_200 <- h_AMISE_n(n = 200, R_K = R_K_N01, R_f2 = R_f2_f01)

# MISE = 200
MISE_200 <- MISE(h, n = 200, w = 1, mu = 1, sigma2 = 1)
h_MISE_200 <- optimize(MISE, c(0, 1), tol = 0.0001, n = 200, w = 1, mu = 1, sigma2 = 1)

# AMISE n = 500
AMISE_500 <- AMISE(n = 500, h = h, R_K = R_K_N01, R_f2 = R_f2_f01)
h_AMISE_500 <- h_AMISE_n(n = 500, R_K = R_K_N01, R_f2 = R_f2_f01)

# MISE = 500
MISE_500 <- MISE(h, n = 500, w = 1, mu = 1, sigma2 = 1)
h_MISE_500 <- optimize(MISE, c(0, 1), tol = 0.0001, n = 500, w = 1, mu = 1, sigma2 = 1)
```

```{r q2plot0, echo=FALSE, fig.width =12, fig.height = 5, fig.cap = "Histograms from sample obtained from n = 100, 200 and 500 for object MW.nm1 \\label{fig:q2plot0}"}
# Histogram plots
par(mfrow = c(1,3))
hist(x100, freq = FALSE, xlim = c(-4,4), main = "Histogram MW.nm1 n = 100")
lines(x, nor1mix::dnorMix(x = x, obj = nor1mix::MW.nm1), col = 2)
hist(x200, freq = FALSE, xlim = c(-4,4), main = "Histogram MW.nm1 n = 200")
lines(x, nor1mix::dnorMix(x = x, obj = nor1mix::MW.nm1), col = 2)
hist(x500, freq = FALSE, xlim = c(-4,4), main = "Histogram MW.nm1 n = 500")
lines(x, nor1mix::dnorMix(x = x, obj = nor1mix::MW.nm1), col = 2)
```

Figure \ref{fig:q2plot1} below shows a graphical representation of the MISE and AMISE related to `nor1mix::MW.nm1` for bandwidth ranges between 0.04 and 0.85. The optimal values of $h_{\text{AMISE}}$ are `r round(h_AMISE_100,3)`, `r round(h_AMISE_200,3)` and `r round(h_AMISE_500,3)`. On the other hand,optimal values of $h_{\text{MISE}}$ are `r round(h_MISE_100$minimum,3)`, `r round(h_MISE_200$minimum,3)` and `r round(h_MISE_500$minimum,3)` for $n = 100, 200, 500$ respectively. In the three curves, we can see that for each sample size, we have that $h_{\text{AMISE}} < h_{\text{MISE}}$. So, the asymptotic approximation underestimate the value of $h_{\text{MISE}}$.

Additionally, when we reduce the bandwidth size, the MISE and AMISE values are quite similar. Additionally, we can see the effect of increasing the number of samples $n$ in the $y$ axis of the plot. (The scale is reduced). So, the difference between both gets closer as the sample size get larger.

In other words, when we increase the number of samples $n$ and reduce the value of the bandwidth, the curves for MISE and AMISE is quite similar.

```{r q2plot1, echo=FALSE, fig.width = 10, fig.height = 5, fig.cap = "MISE and AMISE for range bandwith between 0.04 and 0.85\\label{fig:q2plot1}"}
par(mfrow = c(1,3))
plot(h, MISE_100, type = "l", 
     main = "Comparison MISE(h) and AMISE(h) \n n = 100, nor1mix::MW.nm1",
     ylab = "MISE(h)")
lines(h, AMISE_100, col = "blue")
abline(v = h_AMISE_100, col = "blue")
abline(v = h_MISE_100)
text(0.47, 0.06, labels = paste("h_MISE = ", round(h_MISE_100$minimum,3) ), col = "black", cex = 0.6, font = 2, adj = c(0, 0))
text(0.25, 0.06, labels = paste("h_AMISE = ", round(h_AMISE_100,3) ), col = "blue", cex = 0.6, font = 2, adj = c(0, 0))
legend("topright", legend = c("MISE", "AMISE"), lwd = 1, col = c("black", "blue"))
plot(h, MISE_200, type = "l", 
     main = "Comparison MISE(h) and AMISE(h) \n n = 200, nor1mix::MW.nm1",
     ylab = "MISE(h)")
lines(h, AMISE_200, col = "blue")
abline(v = h_AMISE_200, col = "blue")
abline(v = h_MISE_200)
text(0.4, 0.03, labels = paste("h_MISE = ", round(h_MISE_200$minimum,3) ), col = "black", cex = 0.6, font = 2, adj = c(0, 0))
text(0.2, 0.03, labels = paste("h_AMISE = ", round(h_AMISE_200,3) ), col = "blue", cex = 0.6, font = 2, adj = c(0, 0))
legend("topright", legend = c("MISE", "AMISE"), lwd = 1, col = c("black", "blue"))
plot(h, MISE_500, type = "l", 
     main = "Comparison MISE(h) and AMISE(h) \n n = 500, nor1mix::MW.nm1",
     ylab = "MISE(h)")
lines(h, AMISE_500, col = "blue")
abline(v = h_AMISE_500, col = "blue")
abline(v = h_MISE_500)
text(0.4, 0.01, labels = paste("h_MISE = ", round(h_MISE_500$minimum,3) ), col = "black", cex = 0.5, font = 2, adj = c(0, 0))
text(0.2, 0.01, labels = paste("h_AMISE = ", round(h_AMISE_500,3) ), col = "blue", cex = 0.6, font = 2, adj = c(0, 0))
legend("topright", legend = c("MISE", "AMISE"), lwd = 1, col = c("black", "blue"))
```


```{r eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
# Family two

# Simulating
set.seed(42)
x100 <- nor1mix::rnorMix(n = 100, obj = nor1mix::MW.nm2)
x200 <- nor1mix::rnorMix(n = 200, obj = nor1mix::MW.nm2)
x500 <- nor1mix::rnorMix(n = 500, obj = nor1mix::MW.nm2)

# Parameters:
mu_skewed <-  c(-.3, .3, 1)
sigma2_skewed <- c(1.44, .64, 4/9)
w_skewed <- c(.2, .2, .6)
R_f2_skewed <- R_f2(w = w_skewed, mu = mu_skewed, sigma2 = sigma2_skewed)

# AMISE n = 100

AMISE_100 <- AMISE(n = 100, h = h, R_K = R_K_N01, R_f2 = R_f2_skewed)
h_AMISE_100 <- h_AMISE_n(n = 100, R_K = R_K_N01, R_f2 = R_f2_skewed)

# MISE = 100

MISE_100 <- MISE(h, n = 100, w = w_skewed, mu = mu_skewed, sigma2 = sigma2_skewed)
h_MISE_100 <- optimize(MISE, c(0, 1), tol = 0.0001, n = 100, w = w_skewed, mu = mu_skewed, sigma2 = sigma2_skewed)

# AMISE n = 200

AMISE_200 <- AMISE(n = 200, h = h, R_K = R_K_N01, R_f2 = R_f2_skewed)
h_AMISE_200 <- h_AMISE_n(n = 200, R_K = R_K_N01, R_f2 = R_f2_skewed)

# MISE = 200

MISE_200 <- MISE(h, n = 200, w = w_skewed, mu = mu_skewed, sigma2 = sigma2_skewed)
h_MISE_200 <- optimize(MISE, c(0, 1), tol = 0.0001, n = 200, w = w_skewed, mu = mu_skewed, sigma2 = sigma2_skewed)

# AMISE n = 500

AMISE_500 <- AMISE(n = 500, h = h, R_K = R_K_N01, R_f2 = R_f2_skewed)
h_AMISE_500 <- h_AMISE_n(n = 500, R_K = R_K_N01, R_f2 = R_f2_skewed)

# MISE = 500

MISE_500 <- MISE(h, n = 500, w = w_skewed, mu = mu_skewed, sigma2 = sigma2_skewed)
h_MISE_500 <- optimize(MISE, c(0, 1), tol = 0.0001, n = 500, w = w_skewed, mu = mu_skewed, sigma2 = sigma2_skewed)
```

```{r q2plot2, echo=FALSE, fig.width =12, fig.height = 5, fig.cap = "Histograms from sample obtained from n = 100, 200 and 500 for object MW.nm2 (Skewed) \\label{fig:q2plot2}"}
# Simulating
# Histogram plots
par(mfrow = c(1,3))
hist(x100, freq = FALSE, xlim = c(-4,4), main = "Histogram MW.nm2 n = 100")
lines(x, nor1mix::dnorMix(x = x, obj = nor1mix::MW.nm2), col = 2)
hist(x200, freq = FALSE, xlim = c(-4,4), main = "Histogram MW.nm2 n = 200")
lines(x, nor1mix::dnorMix(x = x, obj = nor1mix::MW.nm2), col = 2)
hist(x500, freq = FALSE, xlim = c(-4,4), main = "Histogram MW.nm2 n = 500")
lines(x, nor1mix::dnorMix(x = x, obj = nor1mix::MW.nm2), col = 2)
```

Figure \ref{fig:q2plot2} shows the histograms for the skewed object `MW.nm2`. In figure \ref{fig:q2plot3} we observe the comparisons of MISE($h$) and AMISE($h$). Similar to the previous graphics, the approximation of these curves is suitable for small $h$ but lower for large $h$. 
However, in this case, we notice how the asymptotic approximation slightly overestimate the value of $h_{\text{MISE}}$, but the difference between $h_{\text{MISE}}$ and $h_{\text{AMISE}}$ are closer than scenario given by `MW.nm1`.

Specifically, values of  $h_{\text{AMISE}}$ are given by `r round(h_AMISE_100,3)`, `r round(h_AMISE_200,3)` and `r round(h_AMISE_500,3)`. On the other hand,optimal values of $h_{\text{MISE}}$ are `r round(h_MISE_100$minimum,3)`, `r round(h_MISE_200$minimum,3)` and `r round(h_MISE_500$minimum,3)` for $n = 100, 200, 500$ respectively.

```{r q2plot3, echo=FALSE, fig.width =12, fig.height = 5, fig.cap = "MISE and AMISE for range bandwith between 0.04 and 0.85\\label{fig:q2plot3}"}
par(mfrow = c(1,3))
plot(h, MISE_100, type = "l", 
     main = "Comparison MISE(h) and AMISE(h) \n n = 100, nor1mix::MW.nm2",
     ylab = "MISE(h)")
lines(h, AMISE_100, col = "blue")
abline(v = h_AMISE_100, col = "blue")
abline(v = h_MISE_100)
text(0.18, 0.06, labels = paste("h_MISE = ", round(h_MISE_100$minimum,3) ), col = "black", cex = 0.6, font = 2, adj = c(0, 0))
text(0.4, 0.06, labels = paste("h_AMISE = ", round(h_AMISE_100,3) ), col = "blue", cex = 0.6, font = 2, adj = c(0, 0))
legend("topright", legend = c("MISE", "AMISE"), lwd = 1, col = c("black", "blue"))
plot(h, MISE_200, type = "l", 
     main = "Comparison MISE(h) and AMISE(h) \n n = 200, nor1mix::MW.nm2",
     ylab = "MISE(h)")
lines(h, AMISE_200, col = "blue")
abline(v = h_AMISE_200, col = "blue")
abline(v = h_MISE_200)
text(0.15, 0.03, labels = paste("h_MISE = ", round(h_MISE_200$minimum,3) ), col = "black", cex = 0.6, font = 2, adj = c(0, 0))
text(0.4, 0.03, labels = paste("h_AMISE = ", round(h_AMISE_200,3) ), col = "blue", cex = 0.6, font = 2, adj = c(0, 0))
legend("topright", legend = c("MISE", "AMISE"), lwd = 1, col = c("black", "blue"))
plot(h, MISE_500, type = "l", 
     main = "Comparison MISE(h) and AMISE(h) \n n = 500, nor1mix::MW.nm2",
     ylab = "MISE(h)")
lines(h, AMISE_500, col = "blue")
abline(v = h_AMISE_500, col = "blue")
abline(v = h_MISE_500)
text(0.3, 0.02, labels = paste("h_MISE = ", round(h_MISE_500$minimum,3) ), col = "black", cex = 0.6, font = 2, adj = c(0, 0))
text(0.1, 0.02, labels = paste("h_AMISE = ", round(h_AMISE_500,3) ), col = "blue", cex = 0.6, font = 2, adj = c(0, 0))
legend("topright", legend = c("MISE", "AMISE"), lwd = 1, col = c("black", "blue"))
```

```{r eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
# Charge to the environment.Do not show results and code

# Simulating
set.seed(42)
x100 <- nor1mix::rnorMix(n = 100, obj = nor1mix::MW.nm6)
x200 <- nor1mix::rnorMix(n = 200, obj = nor1mix::MW.nm6)
x500 <- nor1mix::rnorMix(n = 500, obj = nor1mix::MW.nm6)

mu_Bimodal <-  c(-1, 1)
sigma2_Bimodal <- c(4/9, 4/9)
w_Bimodal <- c(.5, .5)
R_f2_Bimodal <- R_f2(w = w_Bimodal, mu = mu_Bimodal, sigma2 = sigma2_Bimodal)

# AMISE n = 100

AMISE_100 <- AMISE(n = 100, h = h, R_K = R_K_N01, R_f2 = R_f2_Bimodal)
h_AMISE_100 <- h_AMISE_n(n = 100, R_K = R_K_N01, R_f2 = R_f2_Bimodal)

# MISE = 100

MISE_100 <- MISE(h, n = 100, w = w_Bimodal, mu = mu_Bimodal, sigma2 = sigma2_Bimodal)
h_MISE_100 <- optimize(MISE, c(0, 1), tol = 0.0001, n = 100, w = w_Bimodal, mu = mu_Bimodal, sigma2 = sigma2_Bimodal)

AMISE_200 <- AMISE(n = 200, h = h, R_K = R_K_N01, R_f2 = R_f2_Bimodal)
h_AMISE_200 <- h_AMISE_n(n = 200, R_K = R_K_N01, R_f2 = R_f2_Bimodal)

# MISE = 200

MISE_200 <- MISE(h, n = 200, w = w_Bimodal, mu = mu_Bimodal, sigma2 = sigma2_Bimodal)
h_MISE_200 <- optimize(MISE, c(0, 1), tol = 0.0001, n = 200, w = w_Bimodal, mu = mu_Bimodal, sigma2 = sigma2_Bimodal)

AMISE_500 <- AMISE(n = 500, h = h, R_K = R_K_N01, R_f2 = R_f2_Bimodal)
h_AMISE_500 <- h_AMISE_n(n = 500, R_K = R_K_N01, R_f2 = R_f2_Bimodal)

# MISE = 500

MISE_500 <- MISE(h, n = 500, w = w_Bimodal, mu = mu_Bimodal, sigma2 = sigma2_Bimodal)
h_MISE_500 <- optimize(MISE, c(0, 1), tol = 0.0001, n = 500, w = w_Bimodal, mu = mu_Bimodal, sigma2 = sigma2_Bimodal)
```

```{r q2plot4, echo=FALSE, fig.width =12, fig.height = 5, fig.cap = "Histograms from sample obtained from n = 100, 200 and 500 for object MW.nm6 (Bimodal) \\label{fig:q2plot4}"}
# Histogram plots
par(mfrow = c(1,3))
hist(x100, freq = FALSE, xlim = c(-4,4), main = "Histogram MW.nm6 n = 100")
lines(x, nor1mix::dnorMix(x = x, obj = nor1mix::MW.nm6), col = 2)
hist(x200, freq = FALSE, xlim = c(-4,4), main = "Histogram MW.nm6 n = 200")
lines(x, nor1mix::dnorMix(x = x, obj = nor1mix::MW.nm6), col = 2)
hist(x500, freq = FALSE, xlim = c(-4,4), main = "Histogram MW.nm6 n = 500")
lines(x, nor1mix::dnorMix(x = x, obj = nor1mix::MW.nm6), col = 2)


```

For the object `MW.nm6` Figure \ref{fig:q2plot4} shows the histograms for the Bimodal object. Like figure \ref{fig:q2plot0}, increasing the number of observations conducts to a better fitting to the real population.


In figure \ref{fig:q2plot5} we see a similar phenomena than figure \ref{fig:q2plot1}. In this case, the asymptotic MISE underestimate the value for MISE. The minimizers of AMISE($h$) are given by $h_{\text{AMISE}}$ are `r round(h_AMISE_100,3)`, `r round(h_AMISE_200,3)` and `r round(h_AMISE_500,3)`. On the other hand, minimizers of MISE($h$) corresponds to  `r round(h_MISE_100$minimum,3)`, `r round(h_MISE_200$minimum,3)` and `r round(h_MISE_500$minimum,3)` for $n = 100, 200, 500$ respectively.

In this case, we observe how for $n = 500$ the approximation is the same for the first decimal point.

```{r q2plot5, echo=FALSE, fig.width =12, fig.height = 5, fig.cap = "MISE and AMISE for range bandwith between 0.04 and 0.85\\label{fig:q2plot5}"}
par(mfrow = c(1,3))
plot(h, MISE_100, type = "l", 
     main = "Comparison MISE(h) and AMISE(h) \n n = 100, nor1mix::MW.nm6",
     ylab = "MISE(h)")
lines(h, AMISE_100, col = "blue")
abline(v = h_AMISE_100, col = "blue")
abline(v = h_MISE_100)
text(0.4, 0.06, labels = paste("h_MISE = ", round(h_MISE_100$minimum,3) ), col = "black", cex = 0.5, font = 2, adj = c(0, 0))
text(0.1, 0.06, labels = paste("h_AMISE = ", round(h_AMISE_100,3) ), col = "blue", cex = 0.5, font = 2, adj = c(0, 0))
legend("topright", legend = c("MISE", "AMISE"), lwd = 1, col = c("black", "blue"))
plot(h, MISE_200, type = "l", 
     main = "Comparison MISE(h) and AMISE(h) \n n = 200, nor1mix::MW.nm6",
     ylab = "MISE(h)")
lines(h, AMISE_200, col = "blue")
abline(v = h_AMISE_200, col = "blue")
abline(v = h_MISE_200)
text(0.4, 0.03, labels = paste("h_MISE = ", round(h_MISE_200$minimum,3) ), col = "black", cex = 0.5, font = 2, adj = c(0, 0))
text(0.1, 0.03, labels = paste("h_AMISE = ", round(h_AMISE_200,3) ), col = "blue", cex = 0.5, font = 2, adj = c(0, 0))
legend("topright", legend = c("MISE", "AMISE"), lwd = 1, col = c("black", "blue"))
plot(h, MISE_500, type = "l", 
     main = "Comparison MISE(h) and AMISE(h) \n n = 500, nor1mix::MW.nm2",
     ylab = "MISE(h)")
lines(h, AMISE_500, col = "blue")
abline(v = h_AMISE_500, col = "blue")
abline(v = h_MISE_500)
text(0.3, 0.01, labels = paste("h_MISE = ", round(h_MISE_500$minimum,3) ), col = "black", cex = 0.5, font = 2, adj = c(0, 0))
text(0.05, 0.01, labels = paste("h_AMISE = ", round(h_AMISE_500,3) ), col = "blue", cex = 0.5, font = 2, adj = c(0, 0))
legend("topright", legend = c("MISE", "AMISE"), lwd = 1, col = c("black", "blue"))
```

In general terms, we have the following conclusions:

* From equation (10.1), AMISE($h$) $\to \infty$ as $h \to \infty$. However, in the case of MISE($h$), we see how MISE($h$) increase more slowly than AMISE($h$).



\newpage

# Question 3

### Problem Description

Adapt the `np_pred_CI` function to include the argument `type_boot`, which can take either the value `"naive"` or `"wild"`. 

1) If `type_boot = "wild"`, then the function must perform the wild bootstrap algorithm, implemented from scratch following substeps iâ€“iv. 

2) Compare and validate the correct behavior of the confidence intervals, for the two specifications of type_boot, in the model considered in Exercise 5.8 (without performing the full simulation study).

### Response

For this question we will describe briefly the necessary steps related to the implementation of the wild-bootstrap:

First of all, for a given sample with $n$ observations, the  predicted (fitted) values are given by the expression: $\hat{Y}_{i} := \hat{m}(X_{i};q,h)$ with $i = 1,...,n$.

Then, the uncertainty of $\hat{m}(x,q,h)$ (this includes the consideration of new set of samples) can be measure based on two approaches:

1. Asymptotic approach: $\hat{m}(x;q,h) \pm \hat{se}(\hat{m}(x;q,h))$ where $\hat{se}(\hat{m}(x;q,h))$ is the asymptotic estimation of the standard deviation of $\hat{m}(x;q,h)$

2. Bootstrap: In this case, we consider two types of bootstrap:

      - Naive Bootstrap: From the original sample, we create a new set of IID observations called the *Bootstrap sample*
  
      - Wild Bootstrap: This technique is focused on resampling the residuals. It is similar to the *residual bootstrap* in which we fixed the covariate $X_{i}$ and generate a new value of $Y_{i}$ using the fitted model and the *noise* from sampling the residuals. In addition, we generate IID random variables $V_{i}$ (random variable with mean zero and variance 1) and then we can find the perturbed residuals by multiplying this variable against the residults. This modification improves the stability of the computations, especially in the presence of heteroskedasticity.
  
The algorithm of the computation of the wild bootstrap is given in the following steps:

1. Compute $\hat{m}(x;q,h) = \sum_{i=1}^{n} W_{i}^{q}(x)Y_{i}$ from the original sample $(X_{1}, Y_{1}),...,(X_{n}, Y_{n})$
  
2. Enter the wild bootstrap. For $b = 1,...,B$.
  
     i. Simulate $V_{1}^{*b},...,V_{n}^{*b}$ to be iid copies of $V$ such that $E[V] = 0$ and $Var[V] = 1$,
  
     ii. Compute the *pertubed residuals* $e_{i}^{*b} = \hat{e}_{i}V_{i}^{*b}$ where $Y_{i}^{*b} := \hat{m}(X_{i};q,h) + e_{i}^{*b}$ for $i = 1,...,n$.
  
     iii. Obtain the bootstrap sample: $(X_{1}, Y_{1}^{*b}),...,(X_{n}, Y_{n}^{*b})$ where $Y_{i}^{*b} := \hat{m}(X_{i};q,h) + e_{i}^{*b}$ with $i = 1,...,n$.
  
     iv. Compute $\hat{m}^{*b}(x;q,h) = \sum_{i=1}^{n} W_{i}^{q}(x) Y_{i}^{*b}$ from $(X_{1}^{*}, Y_{1}^{*b}),...,(X_{n}^{8}, Y_{n}^{*b})$


As a result, we modified the function `np_pred_CI`. The inputs of the original function are the following:

1. `npfit`: A `np::npreg` object (npfit) 

2. `exdat`: Values of the predictors were to carry out prediction (exdat)

3. `B`:     Number of bootstrap iterations.

4. `conf`:  Range of confidence interval

5. `type_CI`: Type of confidence interval. (Normal standard or quantiles)

Additionally, we add three new inputs:

6. `type_boot`: Type of bootstrap procedure. Options: `Naive` and `Wild` bootstrap

7. `perturbed_res`: For the case of `Wild` bootstrap, we can choose the type of perturbation. As explained before, any random variable with mean $0$ and variance $1$ can be used. As a result, we use two possible perturbation: 

    7.1. `normal`: Based on the normal distribution $V_{i} \sim \mathcal{N}(0,1)$  

    7.2 `golden`: Based on the *golden section binary variable* $P[V = 1 - \phi] = p$, $P[V = \phi] = 1 - p$ and $p = \frac{\phi + 2}{5}$

8. `seed`: Used for reproducibility. Default option is `seed = 42`.

The code of the new version of `np_pred_CI` is given below. We create a new `if` condition, in order to compute the type of bootstrap resampling method chosen by the user. In the case of the `wild` bootstrap, we focus on resampling the residuals obtained from the difference of the fitted value $\hat{Y}_{i}$ and the *real* values $Y_{i}$. Moreover, inside the option `type_boot == wild`, there is an `if` condition with the two possible alternatives for computing the perturbed residuals.

```{r}
# Function to predict and compute confidence intervals for m(x). 

# 1) Inputs

## 1.1) npfit: A np::npreg object (npfit) 
## 1.2) exdat: Values of the predictors where to carry out prediction (exdat)
## 1.3) B:     Number of bootstrap iterations.
## 1.4) conf:  Range of confidence interval
## 1.5) type_CI: Type of confidence interval. (Based on normal standard or quantiles)
## 1.6) type_boot: Type of bootstrap procedure. Options Naive and Wild bootstrap
## 1.7) perturbed_res: Valid only for Wild Bootstrap. Type of perturbation on the residuals. 
## Options are "normal"or "golden"
## 1.8) seed: Used for reproducibility. Default option is seed = 42.

# 2) Outputs

## 2.1) exdat: Values of the predictors where to carry out prediction
## 2.2) m_hat: Predicted regression
## 2.3) lwr:   Lower confidence interval
## 2.4) upr:   Upper confidence interval

np_pred_CI <- function(npfit, 
                       exdat, 
                       B = 200, 
                       conf = 0.95,
                       type_CI = c("standard", "quantiles")[1],
                       type_boot = c("naive", "wild")[1],
                       perturbed_res = c("normal", "golden")[1],
                       seed = 42) {
  
  # Fix seed
  set.seed(seed)
  
  # Extract predictors
  xdat <- npfit$eval
  
  # Extract response, using a trick from np::npplot.rbandwidth
  tt <- terms(npfit$bws)
  tmf <- npfit$bws$call[c(1, match(c("formula", "data"),
                                   names(npfit$bws$call)))]
  tmf[[1]] <- as.name("model.frame")
  tmf[["formula"]] <- tt
  tmf <- eval(tmf, envir = environment(tt))
  ydat <- model.response(tmf)

  # Predictions m_hat from the original sample
  m_hat <- np::npreg(txdat = xdat, 
                     tydat = ydat, 
                     exdat = exdat,
                     bws   = npfit$bws)$mean
  
  if (type_boot == "naive") {
    
    # Function for performing naive bootstrap
    boot_function_naive <- function(data, indices) {
      np::npreg(txdat = xdat[indices,], 
                tydat = ydat[indices],
                exdat = exdat, 
                bws   = npfit$bws)$mean
    }
    
    # Carry out the bootstrap estimator
    m_hat_star <- boot::boot(data = data.frame(xdat), 
                             statistic = boot_function_naive,
                             R = B)$t
    
  } else if (type_boot == "wild") {
    
    # Sample size of the predictors
    n <- length(xdat)
    
    # Y fitted
    Y_hat <- npfit$mean
    
    # Ordinary residuals
    residuals_O <- Y_hat - ydat
    
    # Type of perturbation
    if(perturbed_res == "normal"){
      
      # Function for performing wild bootstrap
      boot_function_wild <- function(data, indices) {
        
        # Step i: Simulate V_{i} copies of V (Mean 0 and variance 1)
        V_n <- rnorm(n)
        
        # Step iii. Obtain the bootstrap sample
        ydat_bt <- Y_hat + data[indices]*V_n
        
        np::npreg(txdat = xdat, 
                  tydat = ydat_bt,
                  exdat = exdat, 
                  bws = npfit$bws)$mean
      }
      
      # Step iv. Carry out the wild bootstrap estimator
      m_hat_star <- boot::boot(data = residuals_O, 
                               statistic = boot_function_wild,
                               R = B)$t
      
    } else if(perturbed_res == "golden"){
      
      # Function for performing wild bootstrap
      boot_function_wild <- function(data, indices) {
        
        # Step i: Simulate V_{i} copies of V (Mean 0 and variance 1)
        phi <- (1 + sqrt(5))/2
        prob <- (phi + 2)/5
        
        golden <- sample(x = c(1-phi,phi), size = n, prob = c(prob, 1 - prob), replace=T)
        
        # Step iii. Obtain the bootstrap sample
        ydat_bt <- Y_hat + data[indices]*golden
        
        np::npreg(txdat = xdat, 
                  tydat = ydat_bt,
                  exdat = exdat, 
                  bws = npfit$bws)$mean
      }
      
      # Step iv. Carry out the wild bootstrap estimator
      m_hat_star <- boot::boot(data = residuals_O, 
                               statistic = boot_function_wild,
                               R = B)$t
      
    }
    
    else{stop("Incorrect type of peturbation")}
    
  }else{stop("Incorrect type_boot")}
  
  # Confidence intervals
  alpha <- 1 - conf
  
  if (type_CI == "standard") {
    
    z <- qnorm(p = 1 - alpha / 2)
    se <- apply(m_hat_star, 2, sd)
    lwr <- m_hat - z * se
    upr <- m_hat + z * se
    
  } else if (type_CI == "quantiles") {
    
    q <- apply(m_hat_star, 2, quantile, probs = c(alpha / 2, 1 - alpha / 2))
    lwr <- q[1, ]
    upr <- q[2, ]
    
  } else {
    stop("Incorrect type_CI")
  }
  # Return evaluation points, estimates, and confidence intervals
  return(data.frame("exdat" = exdat, "m_hat" = m_hat, "lwr" = lwr, "upr" = upr))
}
```

```{r eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
# Charge to the environment.Do not show results and code

#source("R_Code/np_pred_CI.R")
set.seed(12345)
B <- 500
n <- 100
eps <- rnorm(n, sd = 0.75)
m <- function(x) 0.25*x^2 - 0.75*x + 3
X <- rnorm(n, sd = 1.5)
Y <- m(X) + eps

# The spinner can be omitted with
options(np.messages = FALSE)
bw1 <- np::npregbw(formula = Y ~ X, regtype = "lc")
fit1 <- np::npreg(bw1)
```

Finally, we compare and validate the correct behavior of the confidence intervals, for the two specifications of `type_boot` and the two types of perturbations. For this purpose, we simulate the following sample of size $n = 100$ from the regression model $Y = m(x) + \epsilon$ where $m(x) = 0.25x^{2} - 0.75x + 3$, with $X \sim \mathcal{N}(0, 1.5^{2})$ and $\epsilon \sim \mathcal(0, 0.75^{2})$. Figure \ref{fig:q3plot00} shows the simulated sample. In this case the simulated observations are concentrated in the left side of the plot.

```{r q3plot00, echo=FALSE, fig.width = 10, fig.height = 5, fig.cap = "Simulation of 100 observations for m(X). Seed = 12345 \\label{fig:q3plot00}"}
plot(X,Y, main = "Simulate observations m(x)", xlim = c(-5,5))

```

Then, we fit a model using the function `npregbw()` with `regtype = "lc"` and the final model is created with the function `npreg()`. For reference, figures \ref{fig:q3plot0a} and \ref{fig:q3plot0b} shows the confidence interval under normal approximation and quantile compute by `np::npplot`. In particular, we focus on the right side of the confidence intervals. In the case of quantile confidence interval (figure \ref{fig:q3plot0b}) the upper confidence bound is relatively close to the fitted regression $\hat{m}(X_{i};q,h)$

```{r q3plot0a, echo=FALSE, fig.width = 10, fig.height = 5, fig.cap = "Bootstrap intervals: Default option npplot\\label{fig:q3plot0a}"}
par(mfrow = c(1,2))
# Normal approximation confidence intervals + extraction of errors
npplot_std <- plot(fit1, 
                   plot.errors.method = "bootstrap",
                   plot.errors.type = "standard", 
                   plot.errors.boot.num = B,
                   plot.errors.style = "bar", 
                   plot.behavior = "plot-data",
                   lwd = 2,
                   main = "Bootstrap (npplot) \n Normal Approximation Confidence Intervals")
lines(npplot_std$r1$eval[, 1], npplot_std$r1$mean + npplot_std$r1$merr[, 1],
      col = 2, lty = 2)
lines(npplot_std$r1$eval[, 1], npplot_std$r1$mean + npplot_std$r1$merr[, 2],
      col = 2, lty = 2)
```

```{r q3plot0b, echo=FALSE, fig.width = 10, fig.height = 5, fig.cap = "Bootstrap intervals: Default option npplot\\label{fig:q3plot0b}"}
# Quantile confidence intervals + extraction of errors
npplot_qua <- plot(fit1, 
                   plot.errors.method = "bootstrap",
                   plot.errors.type = "quantiles", 
                   plot.errors.boot.num = B,
                   plot.errors.style = "bar", 
                   plot.behavior = "plot-data",
                   main = "Bootstrap (npplot) \n Quantile Confidence Intervals")
lines(npplot_qua$r1$eval[, 1], npplot_qua$r1$mean + npplot_qua$r1$merr[, 1],
      col = 2, lty = 2)
lines(npplot_qua$r1$eval[, 1], npplot_qua$r1$mean + npplot_qua$r1$merr[, 2],
      col = 2, lty = 2)

```

\newpage

Then, we compute $95\%$ confidence intervals for $m(x)$ along `x <- seq(-5, 5, by = 0.1)`. Figure \ref{fig:q3plot1} shows the case of the naive bootstrap. In this case, we see how the confidence intervals of the default quantile option of `np::npplot` coincide perfectly with the confidence interval given by `np_pred_CI` with the option `type_bot = naive`. Additionally, the light blue and green lines represents the extension of the predicted values for $m(x)$ in the grid between $-5$ and $5$.

```{r q3plot1, echo=FALSE, fig.width = 10, fig.height = 5, fig.cap = "Function np_pred_CI: Naive Bootstrap\\label{fig:q3plot1}"}
ci1 <- np_pred_CI(npfit = fit1, 
                  exdat = seq(-5, 5, by = 0.1),
                  B = B, 
                  type_CI = "quantiles",
                  type_boot = "naive")
# Reconstruction of np::npplotâ€™s figure -- the curves coincide perfectly
plot(fit1, 
     plot.errors.method = "bootstrap", 
     plot.errors.type = "quantiles",
     plot.errors.boot.num = B, 
     plot.errors.style = "bar", 
     lwd = 3,  
     main = "Confidence Intervals quantile \n Naive Bootstrap",
     xlim = c(-5,5))
points(X,Y)
lines(npplot_qua$r1$eval[, 1], npplot_qua$r1$mean + npplot_qua$r1$merr[, 1],
      col = 2, lwd = 3)
lines(npplot_qua$r1$eval[, 1], npplot_qua$r1$mean + npplot_qua$r1$merr[, 2],
      col = 2, lwd = 3)
lines(ci1$exdat, ci1$m_hat, col = 3)
lines(ci1$exdat, ci1$lwr, col = 4)
lines(ci1$exdat, ci1$upr, col = 4)
```

Finally, figures \ref{fig:q3plot2} and \ref{fig:q3plot3} shows the confidence intervals in case of wild bootstrap with the normal and golden perturbation. In this case, we notice how the confidence interval is in the function of the number of simulated observations. In the case of the tails, especially on the right side, the confidence intervals are wider in relation to the confidence intervals given by the `naive` bootstrap. On the other hand, in the sections of the graph where there is a concentration of simulated observations, the fitted lower and upper confidence intervals are similar for the two bootstrap methodologies.

```{r q3plot2, echo=FALSE, fig.width = 10, fig.height = 5, fig.cap = "Function np_pred_CI: Wild Bootstrap and normal perturbation\\label{fig:q3plot2}"}
## Wild bootstrap (Normal perturbation)
ci1 <- np_pred_CI(npfit = fit1, 
                  exdat = seq(-5, 5, by = 0.1),
                  B = B, 
                  type_CI = "quantiles",
                  type_boot = "wild")
# Reconstruction of np::npplotâ€™s figure -- the curves coincide perfectly
plot(fit1, 
     plot.errors.method = "bootstrap", 
     plot.errors.type = "quantiles",
     plot.errors.boot.num = B, 
     plot.errors.style = "bar", 
     lwd = 3,
     main = "Confidence Intervals quantile \n Wild Bootstrap - Normal perturbation",
     xlim = c(-5,5) )
points(X,Y)
lines(npplot_qua$r1$eval[, 1], npplot_qua$r1$mean + npplot_qua$r1$merr[, 1],
      col = 2, lwd = 3)
lines(npplot_qua$r1$eval[, 1], npplot_qua$r1$mean + npplot_qua$r1$merr[, 2],
      col = 2, lwd = 3)
lines(ci1$exdat, ci1$m_hat, col = 3)
lines(ci1$exdat, ci1$lwr, col = 4)
lines(ci1$exdat, ci1$upr, col = 4)
```

```{r q3plot3, echo=FALSE, fig.width = 10, fig.height = 5, fig.cap = "Function np_pred_CI: Wild Bootstrap and golden error perturbation\\label{fig:q3plot3}"}
## Wild bootstrap (Golden ratio perturbation)
ci1 <- np_pred_CI(npfit = fit1, 
                  exdat = seq(-5, 5, by = 0.1),
                  B = B, 
                  type_CI = "quantiles",
                  type_boot = "wild",
                  perturbed_res = "golden")
# Reconstruction of np::npplotâ€™s figure -- the curves coincide perfectly
plot(fit1, 
     plot.errors.method = "bootstrap", 
     plot.errors.type = "quantiles",
     plot.errors.boot.num = B, 
     plot.errors.style = "bar", 
     lwd = 3,
     main = "Confidence Intervals quantile \n Wild Bootstrap - Golden ratio perturbation",
     xlim = c(-5,5) )
points(X,Y)
lines(npplot_qua$r1$eval[, 1], npplot_qua$r1$mean + npplot_qua$r1$merr[, 1],
      col = 2, lwd = 3)
lines(npplot_qua$r1$eval[, 1], npplot_qua$r1$mean + npplot_qua$r1$merr[, 2],
      col = 2, lwd = 3)
lines(ci1$exdat, ci1$m_hat, col = 3)
lines(ci1$exdat, ci1$lwr, col = 4)
lines(ci1$exdat, ci1$upr, col = 4)

```